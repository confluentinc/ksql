{
  "version" : "5.5.0",
  "timestamp" : 1581572099067,
  "path" : "query-validation-tests/join-with-custom-timestamp.json",
  "schemas" : {
    "CSAS_S1_JOIN_S2_0.KafkaTopic_Left.Source" : {
      "schema" : "`ROWKEY` BIGINT KEY, `ID` BIGINT, `NAME` STRING, `TS` BIGINT",
      "serdeOptions" : [ ]
    },
    "CSAS_S1_JOIN_S2_0.Join.Right" : {
      "schema" : "`ROWKEY` BIGINT KEY, `S2_ID` BIGINT, `S2_F1` STRING, `S2_F2` STRING, `S2_RTS` BIGINT, `S2_ROWTIME` BIGINT, `S2_ROWKEY` BIGINT",
      "serdeOptions" : [ ]
    },
    "CSAS_S1_JOIN_S2_0.Join.Left" : {
      "schema" : "`ROWKEY` BIGINT KEY, `S1_ID` BIGINT, `S1_NAME` STRING, `S1_TS` BIGINT, `S1_ROWTIME` BIGINT, `S1_ROWKEY` BIGINT",
      "serdeOptions" : [ ]
    },
    "CSAS_S1_JOIN_S2_0.S1_JOIN_S2" : {
      "schema" : "`ROWKEY` BIGINT KEY, `ID` BIGINT, `NAME` STRING, `TS` BIGINT, `F1` STRING, `F2` STRING",
      "serdeOptions" : [ ]
    },
    "CSAS_S1_JOIN_S2_0.KafkaTopic_Right.Source" : {
      "schema" : "`ROWKEY` BIGINT KEY, `ID` BIGINT, `F1` STRING, `F2` STRING, `RTS` BIGINT",
      "serdeOptions" : [ ]
    }
  },
  "testCase" : {
    "name" : "stream stream inner join with ts extractor both sides - PROTOBUF",
    "inputs" : [ {
      "topic" : "s1",
      "key" : 0,
      "value" : {
        "ID" : 0,
        "NAME" : "zero",
        "TS" : 0
      },
      "timestamp" : 0
    }, {
      "topic" : "s2",
      "key" : 0,
      "value" : {
        "ID" : 0,
        "F1" : "blah",
        "F2" : "foo",
        "RTS" : 10000
      },
      "timestamp" : 0
    }, {
      "topic" : "s2",
      "key" : 10,
      "value" : {
        "ID" : 10,
        "F1" : "foo",
        "F2" : "bar",
        "RTS" : 13000
      },
      "timestamp" : 0
    }, {
      "topic" : "s1",
      "key" : 10,
      "value" : {
        "ID" : 10,
        "NAME" : "100",
        "TS" : 11000
      },
      "timestamp" : 0
    }, {
      "topic" : "s1",
      "key" : 0,
      "value" : {
        "ID" : 0,
        "NAME" : "jan",
        "TS" : 8000
      },
      "timestamp" : 0
    } ],
    "outputs" : [ {
      "topic" : "S1_JOIN_S2",
      "key" : 0,
      "value" : {
        "ID" : 0,
        "NAME" : "zero",
        "TS" : 0,
        "F1" : "blah",
        "F2" : "foo"
      },
      "timestamp" : 10000
    }, {
      "topic" : "S1_JOIN_S2",
      "key" : 10,
      "value" : {
        "ID" : 10,
        "NAME" : "100",
        "TS" : 11000,
        "F1" : "foo",
        "F2" : "bar"
      },
      "timestamp" : 13000
    }, {
      "topic" : "S1_JOIN_S2",
      "key" : 0,
      "value" : {
        "ID" : 0,
        "NAME" : "jan",
        "TS" : 8000,
        "F1" : "blah",
        "F2" : "foo"
      },
      "timestamp" : 10000
    } ],
    "topics" : [ {
      "name" : "S1_JOIN_S2",
      "replicas" : 1,
      "numPartitions" : 4
    }, {
      "name" : "s1",
      "schema" : "syntax = \"proto3\";\n\nmessage ConnectDefault1 {\n  int64 ID = 1;\n  string NAME = 2;\n  int64 TS = 3;\n}\n",
      "format" : "PROTOBUF",
      "replicas" : 1,
      "numPartitions" : 4
    }, {
      "name" : "s2",
      "schema" : "syntax = \"proto3\";\n\nmessage ConnectDefault1 {\n  int64 ID = 1;\n  string F1 = 2;\n  string F2 = 3;\n  int64 RTS = 4;\n}\n",
      "format" : "PROTOBUF",
      "replicas" : 1,
      "numPartitions" : 4
    } ],
    "statements" : [ "CREATE STREAM S1 (ROWKEY BIGINT KEY, ID BIGINT, NAME STRING, TS BIGINT) WITH (KAFKA_TOPIC='s1', KEY='ID', TIMESTAMP='TS', VALUE_FORMAT='PROTOBUF');", "CREATE STREAM S2 (ROWKEY BIGINT KEY, ID BIGINT, F1 STRING, F2 STRING, RTS BIGINT) WITH (KAFKA_TOPIC='s2', KEY='ID', TIMESTAMP='RTS', VALUE_FORMAT='PROTOBUF');", "CREATE STREAM S1_JOIN_S2 WITH (TIMESTAMP='TS') AS SELECT\n  S1.ID ID,\n  S1.NAME NAME,\n  S1.TS TS,\n  S2.F1 F1,\n  S2.F2 F2\nFROM S1 S1\nINNER JOIN S2 S2 WITHIN 11 SECONDS ON ((S1.ID = S2.ID))\nEMIT CHANGES;" ],
    "post" : {
      "sources" : [ {
        "name" : "S1_JOIN_S2",
        "type" : "STREAM",
        "schema" : "`ROWKEY` BIGINT KEY, `ID` BIGINT, `NAME` STRING, `TS` BIGINT, `F1` STRING, `F2` STRING",
        "keyFormat" : {
          "format" : "KAFKA"
        },
        "serdeOptions" : [ ]
      }, {
        "name" : "S1",
        "type" : "STREAM",
        "schema" : "`ROWKEY` BIGINT KEY, `ID` BIGINT, `NAME` STRING, `TS` BIGINT",
        "keyFormat" : {
          "format" : "KAFKA"
        },
        "serdeOptions" : [ ]
      }, {
        "name" : "S2",
        "type" : "STREAM",
        "schema" : "`ROWKEY` BIGINT KEY, `ID` BIGINT, `F1` STRING, `F2` STRING, `RTS` BIGINT",
        "keyFormat" : {
          "format" : "KAFKA"
        },
        "serdeOptions" : [ ]
      } ],
      "topics" : {
        "topics" : [ {
          "name" : "_confluent-ksql-some.ksql.service.idquery_CSAS_S1_JOIN_S2_0-KSTREAM-JOINOTHER-0000000009-store-changelog",
          "keyFormat" : {
            "formatInfo" : {
              "format" : "KAFKA"
            }
          },
          "valueFormat" : {
            "format" : "PROTOBUF"
          }
        }, {
          "name" : "s2",
          "keyFormat" : {
            "formatInfo" : {
              "format" : "KAFKA"
            }
          },
          "valueFormat" : {
            "format" : "PROTOBUF"
          },
          "partitions" : 4
        }, {
          "name" : "s1",
          "keyFormat" : {
            "formatInfo" : {
              "format" : "KAFKA"
            }
          },
          "valueFormat" : {
            "format" : "PROTOBUF"
          },
          "partitions" : 4
        }, {
          "name" : "S1_JOIN_S2",
          "keyFormat" : {
            "formatInfo" : {
              "format" : "KAFKA"
            }
          },
          "valueFormat" : {
            "format" : "PROTOBUF"
          },
          "partitions" : 4
        }, {
          "name" : "_confluent-ksql-some.ksql.service.idquery_CSAS_S1_JOIN_S2_0-KSTREAM-JOINTHIS-0000000008-store-changelog",
          "keyFormat" : {
            "formatInfo" : {
              "format" : "KAFKA"
            }
          },
          "valueFormat" : {
            "format" : "PROTOBUF"
          }
        } ]
      }
    }
  }
}